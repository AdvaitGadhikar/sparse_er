{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bf29397-5197-41ec-8c91-e9818883f3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/advaitgadhikar/opt/anaconda3/envs/experiments/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "445be5d2-e8a4-480c-8ed8-00f73bcefdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: folktables in /Users/advaitgadhikar/opt/anaconda3/envs/experiments/lib/python3.10/site-packages (0.0.12)\n",
      "Requirement already satisfied: requests in /Users/advaitgadhikar/opt/anaconda3/envs/experiments/lib/python3.10/site-packages (from folktables) (2.28.1)\n",
      "Requirement already satisfied: pandas in /Users/advaitgadhikar/opt/anaconda3/envs/experiments/lib/python3.10/site-packages (from folktables) (1.5.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/advaitgadhikar/opt/anaconda3/envs/experiments/lib/python3.10/site-packages (from folktables) (1.1.2)\n",
      "Requirement already satisfied: numpy in /Users/advaitgadhikar/opt/anaconda3/envs/experiments/lib/python3.10/site-packages (from folktables) (1.23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/advaitgadhikar/opt/anaconda3/envs/experiments/lib/python3.10/site-packages (from pandas->folktables) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/advaitgadhikar/opt/anaconda3/envs/experiments/lib/python3.10/site-packages (from pandas->folktables) (2022.2.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/advaitgadhikar/opt/anaconda3/envs/experiments/lib/python3.10/site-packages (from requests->folktables) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/advaitgadhikar/opt/anaconda3/envs/experiments/lib/python3.10/site-packages (from requests->folktables) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/advaitgadhikar/opt/anaconda3/envs/experiments/lib/python3.10/site-packages (from requests->folktables) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/advaitgadhikar/opt/anaconda3/envs/experiments/lib/python3.10/site-packages (from requests->folktables) (2022.12.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/advaitgadhikar/opt/anaconda3/envs/experiments/lib/python3.10/site-packages (from scikit-learn->folktables) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/advaitgadhikar/opt/anaconda3/envs/experiments/lib/python3.10/site-packages (from scikit-learn->folktables) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/advaitgadhikar/opt/anaconda3/envs/experiments/lib/python3.10/site-packages (from scikit-learn->folktables) (1.9.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/advaitgadhikar/opt/anaconda3/envs/experiments/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->folktables) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install folktables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa44184a-78ff-4406-966e-aaaa6926d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktables import ACSDataSource, ACSEmployment\n",
    "\n",
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data(states=[\"AL\"], download=True)\n",
    "features, label, group = ACSEmployment.df_to_numpy(acs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f2ef789-44dd-474d-8056-b75ad723fe5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38221, 16) (38221,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([22583.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0., 15638.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGdCAYAAADwjmIIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAh3ElEQVR4nO3deXDU9f3H8VcS2A0ouwExCSmRsxAuQYOERdGiGQKkKJWOXEMjjVA1OEKUSyjwEysUT1SOwSt2BuRwhCrQQAwCBQJIJJUrVCQUGNyAAlmIkhDy/f3RyVe3BGVjDvfj8zGzM+b7fe/uZz+i+5zN7hJiWZYlAAAAw4TW9QIAAABqApEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEj16noBdam8vFwnT55Uo0aNFBISUtfLAQAA18CyLJ0/f14xMTEKDb366zW/6Mg5efKkYmNj63oZAACgCo4fP67mzZtf9fwvOnIaNWok6b+b5HK56ng1AADgWvh8PsXGxtrP41fzi46cil9RuVwuIgcAgCDzY2814Y3HAADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwUr26XoCpWk5eW9dLCNjROcl1vQQAAKoNr+QAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwUkCRM3v2bN12221q1KiRIiMjNWjQIB06dMhv5uLFi0pLS9MNN9yg66+/XoMHD1ZhYaHfzLFjx5ScnKyGDRsqMjJSEyZMUFlZmd/Mpk2bdOutt8rpdKpt27bKyMi4Yj3z589Xy5YtFR4eroSEBO3atSuQhwMAAAwWUORs3rxZaWlp2rFjh7KysnTp0iX17dtXxcXF9sz48eP14YcfauXKldq8ebNOnjyp+++/3z5/+fJlJScnq7S0VNu3b9c777yjjIwMTZ8+3Z4pKChQcnKy+vTpo7y8PI0bN04PPfSQ1q9fb88sX75c6enpmjFjhj799FN17dpVSUlJOnXq1E/ZDwAAYIgQy7Ksql759OnTioyM1ObNm3XnnXeqqKhIN954o5YuXarf//73kqT8/Hx16NBBOTk56tmzp/7xj3/ot7/9rU6ePKmoqChJ0qJFizRp0iSdPn1aDodDkyZN0tq1a7Vv3z77voYOHapz584pMzNTkpSQkKDbbrtNr732miSpvLxcsbGxeuyxxzR58uRrWr/P55Pb7VZRUZFcLldVt6FSLSevrdbbqw1H5yTX9RIAAPhR1/r8/ZPek1NUVCRJatKkiSQpNzdXly5dUmJioj0TFxenm266STk5OZKknJwcdenSxQ4cSUpKSpLP59P+/fvtme/fRsVMxW2UlpYqNzfXbyY0NFSJiYn2DAAA+GWrV9UrlpeXa9y4cbr99tvVuXNnSZLX65XD4VBERITfbFRUlLxerz3z/cCpOF9x7odmfD6fvv32W509e1aXL1+udCY/P/+qay4pKVFJSYn9s8/nC+ARAwCAYFLlV3LS0tK0b98+LVu2rDrXU6Nmz54tt9ttX2JjY+t6SQAAoIZUKXLGjh2rNWvW6OOPP1bz5s3t49HR0SotLdW5c+f85gsLCxUdHW3P/O+nrSp+/rEZl8ulBg0aqGnTpgoLC6t0puI2KjNlyhQVFRXZl+PHjwf2wAEAQNAIKHIsy9LYsWO1atUqbdy4Ua1atfI7Hx8fr/r16ys7O9s+dujQIR07dkwej0eS5PF4tHfvXr9PQWVlZcnlcqljx472zPdvo2Km4jYcDofi4+P9ZsrLy5WdnW3PVMbpdMrlcvldAACAmQJ6T05aWpqWLl2qv//972rUqJH9Hhq3260GDRrI7XYrNTVV6enpatKkiVwulx577DF5PB717NlTktS3b1917NhRI0eO1Ny5c+X1ejVt2jSlpaXJ6XRKkh5++GG99tprmjhxov74xz9q48aNWrFihdau/e4TS+np6UpJSVH37t3Vo0cPvfzyyyouLtaoUaOqa28AAEAQCyhyFi5cKEn6zW9+43f87bff1oMPPihJeumllxQaGqrBgwerpKRESUlJWrBggT0bFhamNWvW6JFHHpHH49F1112nlJQUPf300/ZMq1attHbtWo0fP17z5s1T8+bN9cYbbygpKcmeGTJkiE6fPq3p06fL6/WqW7duyszMvOLNyAAA4JfpJ31PTrDje3L88T05AIBgUCvfkwMAAPBzReQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAj1avrBQAAgB/XcvLaul5CwI7OSa7T++eVHAAAYCQiBwAAGInIAQAARiJyAACAkQKOnC1btmjgwIGKiYlRSEiIVq9e7Xf+wQcfVEhIiN+lX79+fjNnzpzRiBEj5HK5FBERodTUVF24cMFv5rPPPlPv3r0VHh6u2NhYzZ0794q1rFy5UnFxcQoPD1eXLl20bt26QB8OAAAwVMCRU1xcrK5du2r+/PlXnenXr5++/PJL+/Luu+/6nR8xYoT279+vrKwsrVmzRlu2bNGYMWPs8z6fT3379lWLFi2Um5ur5557TjNnztTixYvtme3bt2vYsGFKTU3Vnj17NGjQIA0aNEj79u0L9CEBAAADBfwR8v79+6t///4/OON0OhUdHV3puYMHDyozM1OffPKJunfvLkl69dVXNWDAAD3//POKiYnRkiVLVFpaqrfeeksOh0OdOnVSXl6eXnzxRTuG5s2bp379+mnChAmSpFmzZikrK0uvvfaaFi1aFOjDAgAAhqmR9+Rs2rRJkZGRat++vR555BF9/fXX9rmcnBxFRETYgSNJiYmJCg0N1c6dO+2ZO++8Uw6Hw55JSkrSoUOHdPbsWXsmMTHR736TkpKUk5Nz1XWVlJTI5/P5XQAAgJmqPXL69eunv/3tb8rOztZf//pXbd68Wf3799fly5clSV6vV5GRkX7XqVevnpo0aSKv12vPREVF+c1U/PxjMxXnKzN79my53W77Ehsb+9MeLAAA+Nmq9m88Hjp0qP3PXbp00c0336w2bdpo06ZNuueee6r77gIyZcoUpaen2z/7fD5CBwAAQ9X4R8hbt26tpk2b6vDhw5Kk6OhonTp1ym+mrKxMZ86csd/HEx0drcLCQr+Zip9/bOZq7wWS/vteIZfL5XcBAABmqvHIOXHihL7++ms1a9ZMkuTxeHTu3Dnl5ubaMxs3blR5ebkSEhLsmS1btujSpUv2TFZWltq3b6/GjRvbM9nZ2X73lZWVJY/HU9MPCQAABIGAI+fChQvKy8tTXl6eJKmgoEB5eXk6duyYLly4oAkTJmjHjh06evSosrOzdd9996lt27ZKSkqSJHXo0EH9+vXT6NGjtWvXLm3btk1jx47V0KFDFRMTI0kaPny4HA6HUlNTtX//fi1fvlzz5s3z+1XT448/rszMTL3wwgvKz8/XzJkztXv3bo0dO7YatgUAAAS7gCNn9+7duuWWW3TLLbdIktLT03XLLbdo+vTpCgsL02effaZ7771X7dq1U2pqquLj4/XPf/5TTqfTvo0lS5YoLi5O99xzjwYMGKA77rjD7ztw3G63NmzYoIKCAsXHx+uJJ57Q9OnT/b5Lp1evXlq6dKkWL16srl276r333tPq1avVuXPnn7IfAADAECGWZVl1vYi64vP55Ha7VVRUVO3vz2k5eW213l5tODonua6XAAC4Cp5XvnOtz9/83VUAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjBRw5GzZskUDBw5UTEyMQkJCtHr1ar/zlmVp+vTpatasmRo0aKDExER9/vnnfjNnzpzRiBEj5HK5FBERodTUVF24cMFv5rPPPlPv3r0VHh6u2NhYzZ0794q1rFy5UnFxcQoPD1eXLl20bt26QB8OAAAwVMCRU1xcrK5du2r+/PmVnp87d65eeeUVLVq0SDt37tR1112npKQkXbx40Z4ZMWKE9u/fr6ysLK1Zs0ZbtmzRmDFj7PM+n099+/ZVixYtlJubq+eee04zZ87U4sWL7Znt27dr2LBhSk1N1Z49ezRo0CANGjRI+/btC/QhAQAAA4VYlmVV+cohIVq1apUGDRok6b+v4sTExOiJJ57Qk08+KUkqKipSVFSUMjIyNHToUB08eFAdO3bUJ598ou7du0uSMjMzNWDAAJ04cUIxMTFauHChpk6dKq/XK4fDIUmaPHmyVq9erfz8fEnSkCFDVFxcrDVr1tjr6dmzp7p166ZFixZd0/p9Pp/cbreKiorkcrmqug2Vajl5bbXeXm04Oie5rpcAALgKnle+c63P39X6npyCggJ5vV4lJibax9xutxISEpSTkyNJysnJUUREhB04kpSYmKjQ0FDt3LnTnrnzzjvtwJGkpKQkHTp0SGfPnrVnvn8/FTMV91OZkpIS+Xw+vwsAADBTtUaO1+uVJEVFRfkdj4qKss95vV5FRkb6na9Xr56aNGniN1PZbXz/Pq42U3G+MrNnz5bb7bYvsbGxgT5EAAAQJH5Rn66aMmWKioqK7Mvx48frekkAAKCGVGvkREdHS5IKCwv9jhcWFtrnoqOjderUKb/zZWVlOnPmjN9MZbfx/fu42kzF+co4nU65XC6/CwAAMFO1Rk6rVq0UHR2t7Oxs+5jP59POnTvl8XgkSR6PR+fOnVNubq49s3HjRpWXlyshIcGe2bJliy5dumTPZGVlqX379mrcuLE98/37qZipuB8AAPDLFnDkXLhwQXl5ecrLy5P03zcb5+Xl6dixYwoJCdG4ceP0zDPP6IMPPtDevXv1hz/8QTExMfYnsDp06KB+/fpp9OjR2rVrl7Zt26axY8dq6NChiomJkSQNHz5cDodDqamp2r9/v5YvX6558+YpPT3dXsfjjz+uzMxMvfDCC8rPz9fMmTO1e/dujR079qfvCgAACHr1Ar3C7t271adPH/vnivBISUlRRkaGJk6cqOLiYo0ZM0bnzp3THXfcoczMTIWHh9vXWbJkicaOHat77rlHoaGhGjx4sF555RX7vNvt1oYNG5SWlqb4+Hg1bdpU06dP9/sunV69emnp0qWaNm2annrqKf3617/W6tWr1blz5yptBAAAMMtP+p6cYMf35Pjje3IA4OeL55Xv1Mn35AAAAPxcEDkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMFK1R87MmTMVEhLid4mLi7PPX7x4UWlpabrhhht0/fXXa/DgwSosLPS7jWPHjik5OVkNGzZUZGSkJkyYoLKyMr+ZTZs26dZbb5XT6VTbtm2VkZFR3Q8FAAAEsRp5JadTp0768ssv7cvWrVvtc+PHj9eHH36olStXavPmzTp58qTuv/9++/zly5eVnJys0tJSbd++Xe+8844yMjI0ffp0e6agoEDJycnq06eP8vLyNG7cOD300ENav359TTwcAAAQhOrVyI3Wq6fo6OgrjhcVFenNN9/U0qVLdffdd0uS3n77bXXo0EE7duxQz549tWHDBh04cEAfffSRoqKi1K1bN82aNUuTJk3SzJkz5XA4tGjRIrVq1UovvPCCJKlDhw7aunWrXnrpJSUlJdXEQwIAAEGmRl7J+fzzzxUTE6PWrVtrxIgROnbsmCQpNzdXly5dUmJioj0bFxenm266STk5OZKknJwcdenSRVFRUfZMUlKSfD6f9u/fb898/zYqZipu42pKSkrk8/n8LgAAwEzVHjkJCQnKyMhQZmamFi5cqIKCAvXu3Vvnz5+X1+uVw+FQRESE33WioqLk9XolSV6v1y9wKs5XnPuhGZ/Pp2+//faqa5s9e7bcbrd9iY2N/akPFwAA/ExV+6+r+vfvb//zzTffrISEBLVo0UIrVqxQgwYNqvvuAjJlyhSlp6fbP/t8PkIHAABD1fhHyCMiItSuXTsdPnxY0dHRKi0t1blz5/xmCgsL7ffwREdHX/Fpq4qff2zG5XL9YEg5nU65XC6/CwAAMFONR86FCxf0xRdfqFmzZoqPj1f9+vWVnZ1tnz906JCOHTsmj8cjSfJ4PNq7d69OnTplz2RlZcnlcqljx472zPdvo2Km4jYAAACqPXKefPJJbd68WUePHtX27dv1u9/9TmFhYRo2bJjcbrdSU1OVnp6ujz/+WLm5uRo1apQ8Ho969uwpSerbt686duyokSNH6l//+pfWr1+vadOmKS0tTU6nU5L08MMP68iRI5o4caLy8/O1YMECrVixQuPHj6/uhwMAAIJUtb8n58SJExo2bJi+/vpr3Xjjjbrjjju0Y8cO3XjjjZKkl156SaGhoRo8eLBKSkqUlJSkBQsW2NcPCwvTmjVr9Mgjj8jj8ei6665TSkqKnn76aXumVatWWrt2rcaPH6958+apefPmeuONN/j4OAAAsIVYlmXV9SLqis/nk9vtVlFRUbW/P6fl5LXVenu14eic5LpeAgDgKnhe+c61Pn/zd1cBAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMFLQR878+fPVsmVLhYeHKyEhQbt27arrJQEAgJ+BoI6c5cuXKz09XTNmzNCnn36qrl27KikpSadOnarrpQEAgDoW1JHz4osvavTo0Ro1apQ6duyoRYsWqWHDhnrrrbfqemkAAKCO1avrBVRVaWmpcnNzNWXKFPtYaGioEhMTlZOTU+l1SkpKVFJSYv9cVFQkSfL5fNW+vvKSb6r9NmtaTewDAKB68Lxy5e1alvWDc0EbOV999ZUuX76sqKgov+NRUVHKz8+v9DqzZ8/W//3f/11xPDY2tkbWGGzcL9f1CgAAJqnp55Xz58/L7XZf9XzQRk5VTJkyRenp6fbP5eXlOnPmjG644QaFhIRU2/34fD7Fxsbq+PHjcrlc1Xa78Mc+1x72unawz7WDfa4dNbnPlmXp/PnziomJ+cG5oI2cpk2bKiwsTIWFhX7HCwsLFR0dXel1nE6nnE6n37GIiIiaWqJcLhf/AdUC9rn2sNe1g32uHexz7aipff6hV3AqBO0bjx0Oh+Lj45WdnW0fKy8vV3Z2tjweTx2uDAAA/BwE7Ss5kpSenq6UlBR1795dPXr00Msvv6zi4mKNGjWqrpcGAADqWFBHzpAhQ3T69GlNnz5dXq9X3bp1U2Zm5hVvRq5tTqdTM2bMuOJXY6he7HPtYa9rB/tcO9jn2vFz2OcQ68c+fwUAABCEgvY9OQAAAD+EyAEAAEYicgAAgJGIHAAAYCQip4rmz5+vli1bKjw8XAkJCdq1a9cPzq9cuVJxcXEKDw9Xly5dtG7dulpaaXALZJ9ff/119e7dW40bN1bjxo2VmJj4o/9e8F+B/nmusGzZMoWEhGjQoEE1u0CDBLrX586dU1pampo1ayan06l27drx/49rEOg+v/zyy2rfvr0aNGig2NhYjR8/XhcvXqyl1QanLVu2aODAgYqJiVFISIhWr179o9fZtGmTbr31VjmdTrVt21YZGRk1u0gLAVu2bJnlcDist956y9q/f781evRoKyIiwiosLKx0ftu2bVZYWJg1d+5c68CBA9a0adOs+vXrW3v37q3llQeXQPd5+PDh1vz58609e/ZYBw8etB588EHL7XZbJ06cqOWVB5dA97lCQUGB9atf/crq3bu3dd9999XOYoNcoHtdUlJide/e3RowYIC1detWq6CgwNq0aZOVl5dXyysPLoHu85IlSyyn02ktWbLEKigosNavX281a9bMGj9+fC2vPLisW7fOmjp1qvX+++9bkqxVq1b94PyRI0eshg0bWunp6daBAwesV1991QoLC7MyMzNrbI1EThX06NHDSktLs3++fPmyFRMTY82ePbvS+QceeMBKTk72O5aQkGD96U9/qtF1BrtA9/l/lZWVWY0aNbLeeeedmlqiEaqyz2VlZVavXr2sN954w0pJSSFyrlGge71w4UKrdevWVmlpaW0t0QiB7nNaWpp19913+x1LT0+3br/99hpdp0muJXImTpxoderUye/YkCFDrKSkpBpbF7+uClBpaalyc3OVmJhoHwsNDVViYqJycnIqvU5OTo7fvCQlJSVddR5V2+f/9c033+jSpUtq0qRJTS0z6FV1n59++mlFRkYqNTW1NpZphKrs9QcffCCPx6O0tDRFRUWpc+fOevbZZ3X58uXaWnbQqco+9+rVS7m5ufavtI4cOaJ169ZpwIABtbLmX4q6eC4M6m88rgtfffWVLl++fMW3KkdFRSk/P7/S63i93krnvV5vja0z2FVln//XpEmTFBMTc8V/VPhOVfZ569atevPNN5WXl1cLKzRHVfb6yJEj2rhxo0aMGKF169bp8OHDevTRR3Xp0iXNmDGjNpYddKqyz8OHD9dXX32lO+64Q5ZlqaysTA8//LCeeuqp2ljyL8bVngt9Pp++/fZbNWjQoNrvk1dyYKQ5c+Zo2bJlWrVqlcLDw+t6OcY4f/68Ro4cqddff11Nmzat6+UYr7y8XJGRkVq8eLHi4+M1ZMgQTZ06VYsWLarrpRll06ZNevbZZ7VgwQJ9+umnev/997V27VrNmjWrrpeGn4hXcgLUtGlThYWFqbCw0O94YWGhoqOjK71OdHR0QPOo2j5XeP755zVnzhx99NFHuvnmm2tymUEv0H3+4osvdPToUQ0cONA+Vl5eLkmqV6+eDh06pDZt2tTsooNUVf5MN2vWTPXr11dYWJh9rEOHDvJ6vSotLZXD4ajRNQejquzzn//8Z40cOVIPPfSQJKlLly4qLi7WmDFjNHXqVIWG8npAdbjac6HL5aqRV3EkXskJmMPhUHx8vLKzs+1j5eXlys7OlsfjqfQ6Ho/Hb16SsrKyrjqPqu2zJM2dO1ezZs1SZmamunfvXhtLDWqB7nNcXJz27t2rvLw8+3LvvfeqT58+ysvLU2xsbG0uP6hU5c/07bffrsOHD9shKUn//ve/1axZMwLnKqqyz998880VIVMRlhZ/vWO1qZPnwhp7S7PBli1bZjmdTisjI8M6cOCANWbMGCsiIsLyer2WZVnWyJEjrcmTJ9vz27Zts+rVq2c9//zz1sGDB60ZM2bwEfJrEOg+z5kzx3I4HNZ7771nffnll/bl/PnzdfUQgkKg+/y/+HTVtQt0r48dO2Y1atTIGjt2rHXo0CFrzZo1VmRkpPXMM8/U1UMICoHu84wZM6xGjRpZ7777rnXkyBFrw4YNVps2bawHHnigrh5CUDh//ry1Z88ea8+ePZYk68UXX7T27Nlj/ec//7Esy7ImT55sjRw50p6v+Aj5hAkTrIMHD1rz58/nI+Q/V6+++qp10003WQ6Hw+rRo4e1Y8cO+9xdd91lpaSk+M2vWLHCateuneVwOKxOnTpZa9eureUVB6dA9rlFixaWpCsuM2bMqP2FB5lA/zx/H5ETmED3evv27VZCQoLldDqt1q1bW3/5y1+ssrKyWl518Alkny9dumTNnDnTatOmjRUeHm7FxsZajz76qHX27NnaX3gQ+fjjjyv9f27F3qakpFh33XXXFdfp1q2b5XA4rNatW1tvv/12ja4xxLJ4LQ4AAJiH9+QAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACM9P8h4W96Tg1iTwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    features, label, group, test_size=0.2, random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "plt.hist(y_train.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5420f2aa-95f6-4a02-93a7-1bafb4c73a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from folktables import ACSDataSource, ACSIncome\n",
    "\n",
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "ca_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "\n",
    "ca_features, ca_labels, _ = ACSIncome.df_to_pandas(ca_data)\n",
    "\n",
    "ca_features.to_csv('ca_features.csv', index=False)\n",
    "ca_labels.to_csv('ca_labels.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ae069e1-b79d-4931-acb3-88ad219a9408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195665, 10)\n"
     ]
    }
   ],
   "source": [
    "ca_features = np.array(ca_features)\n",
    "ca_labels = np.array(ca_labels).astype(int).squeeze()\n",
    "print(ca_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22e35a13-f533-47c5-8a14-cd63453fff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch.multiprocessing\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "# import h5py\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Fairness(Dataset):\n",
    "\n",
    "    def __init__(self, X_train, X_test, y_train, y_test, train = True):\n",
    "        if train:\n",
    "            self.data = X_train\n",
    "            self.labels = y_train\n",
    "        else:\n",
    "            self.data = X_test\n",
    "            self.labels = y_test\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index,:], self.labels[index].squeeze()\n",
    "\n",
    "train_dataset = Fairness(X_train, X_test, y_train, y_test, train=True)\n",
    "val_dataset = Fairness(X_train, X_test, y_train, y_test, train=False)\n",
    "\n",
    "\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2dc6681-e06c-4f29-b783-918c7236fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import init\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class LinearER(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(LinearER, self).__init__(in_features, out_features, bias)\n",
    "        self.register_buffer('weight_mask', torch.ones(self.weight.shape))\n",
    "        if self.bias is not None:\n",
    "            self.register_buffer('bias_mask', torch.ones(self.bias.shape))\n",
    "    \n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.weight_mask = self.weight_mask.to(self.weight.device)\n",
    "        W = self.weight_mask * self.weight\n",
    "        if self.bias is not None:\n",
    "            b = self.bias_mask * self.bias\n",
    "        else:\n",
    "            b = self.bias\n",
    "        return F.linear(input, W, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9230dfe8-4ac8-48df-a8d5-daf05a0fba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, nin = 16, nout = 2, width = 256):\n",
    "        \n",
    "        super(ToyModel, self).__init__()\n",
    "        \n",
    "        self.linear1 = LinearER(nin, width)\n",
    "        self.linear2 = LinearER(width, width)\n",
    "        self.linear3 = LinearER(width, width)\n",
    "        self.linear4 = LinearER(width, nout)\n",
    "        self.bn1 = nn.BatchNorm1d(width)\n",
    "        self.bn2 = nn.BatchNorm1d(width)\n",
    "        self.bn3 = nn.BatchNorm1d(width)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(F.relu(self.linear1(x)))\n",
    "        x = self.bn2(F.relu(self.linear2(x)))        \n",
    "        x = self.bn3(F.relu(self.linear3(x)))        \n",
    "        x = (self.linear4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f166980-5509-4891-bf52-6351bc61f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_mag(model, density):\n",
    "    score_list = {}\n",
    "    for n, m in model.named_modules():\n",
    "        # torch.cat([torch.flatten(v) for v in self.scores.values()])\n",
    "        if isinstance(m, (nn.Linear)):\n",
    "            score_list[n] = (m.weight_mask.to(m.weight.device) * m.weight).detach().abs_()\n",
    "\n",
    "    global_scores = torch.cat([torch.flatten(v) for v in score_list.values()])\n",
    "    k = int((1 - density) * global_scores.numel())\n",
    "    threshold, _ = torch.kthvalue(global_scores, k)\n",
    "\n",
    "    if not k < 1:\n",
    "        total_num = 0\n",
    "        total_den = 0\n",
    "        for n, m in model.named_modules():\n",
    "            if isinstance(m, (nn.Linear)):\n",
    "                score = score_list[n].to(m.weight.device)\n",
    "                zero = torch.tensor([0.]).to(m.weight.device)\n",
    "                one = torch.tensor([1.]).to(m.weight.device)\n",
    "                m.weight_mask = torch.where(score <= threshold, zero, one)\n",
    "                total_num += (m.weight_mask == 1).sum()\n",
    "                total_den += m.weight_mask.numel()\n",
    "\n",
    "    print('Overall model density after magnitude pruning at current iteration = ', total_num / total_den)\n",
    "    return model\n",
    "\n",
    "def prune_random(model, density):\n",
    "    total_num = 0\n",
    "    total_den = 0\n",
    "\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, (nn.Linear)):\n",
    "            score = (m.weight_mask.to(m.weight.device) * torch.randn_like(m.weight).to(m.weight.device)).detach().abs_()\n",
    "            global_scores = torch.flatten(score)\n",
    "            k = int((1 - density) * global_scores.numel())\n",
    "            if k == 0:\n",
    "                threshold = 0\n",
    "            else: \n",
    "                threshold, _ = torch.kthvalue(global_scores, k)\n",
    "            print('Layer', n, ' params ', k, global_scores.numel())\n",
    "\n",
    "            score = score.to(m.weight.device)\n",
    "            zero = torch.tensor([0.]).to(m.weight.device)\n",
    "            one = torch.tensor([1.]).to(m.weight.device)\n",
    "            m.weight_mask = torch.where(score <= threshold, zero, one)\n",
    "            total_num += (m.weight_mask == 1).sum()\n",
    "            total_den += m.weight_mask.numel()\n",
    "    print('Overall model density after magnitude pruning at current iteration = ', total_num / total_den)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cfd3f85-dd63-4858-a4de-b7d9e2bbc892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import time\n",
    "def write_result_to_csv(**kwargs):\n",
    "    filename = 'tabular_results.csv'\n",
    "    base_dir = ''\n",
    "\n",
    "    results = pathlib.Path(base_dir, filename)\n",
    "    \n",
    "    if not results.exists():\n",
    "        print('creating and opening a results file')\n",
    "        with open(results, 'w', newline=''):\n",
    "\n",
    "            results.write_text(\n",
    "                \"Name,\"\n",
    "                \"Best Val Top 1, \"\n",
    "                \"Sparsity\\n\"\n",
    "            )\n",
    "\n",
    "    now = time.strftime(\"%m-%d-%y_%H:%M:%S\")\n",
    "\n",
    "    with open(results, \"a+\") as f:\n",
    "        f.write(\n",
    "            (\n",
    "            \"{name},\"\n",
    "            \"{best_acc1:.04f},\"\n",
    "            \"{sparsity:.04f}\\n\"\n",
    "            ).format(**kwargs)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4548d29a-a847-435c-ace7-8cfbbe8bbb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "def run(pruner, threshold_list, name, er = 0.5, reset_weight=False):\n",
    "    model = ToyModel(width = 256)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-2)\n",
    "    \n",
    "    #init ER density is 1.0 i.e starting from a ful dense network\n",
    "    density = er\n",
    "\n",
    "    torch.save(model.state_dict(),\"model_tabular.pt\")\n",
    "    torch.save(optimizer.state_dict(),\"optimizer_tabular.pt\")\n",
    "\n",
    "    for n,m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.weight_mask = torch.zeros_like(m.weight).bernoulli_(p = density)\n",
    "\n",
    "    # threshold_list = [0.4]\n",
    "    test_acc = []\n",
    "    for d in threshold_list:\n",
    "        for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "            running_loss = 0.0\n",
    "            model.train()\n",
    "            for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = inputs.to(torch.float32), labels.long()\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward + backward + optimize\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if i % 100 == 99:    # print every 2000 mini-batches\n",
    "                    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                    running_loss = 0.0\n",
    "\n",
    "            model.eval()\n",
    "            val_acc = 0\n",
    "            val_loss = 0\n",
    "            cnt = 0\n",
    "            for i, (inputs, labels) in enumerate(testloader):\n",
    "                inputs, labels = inputs.to(torch.float32), labels.long()\n",
    "                preds = model(inputs)\n",
    "                # print(preds.argmax(dim=1), labels)\n",
    "                # print(preds)\n",
    "                val_loss += criterion(preds, labels)\n",
    "                val_acc += (preds.argmax(dim=1) == labels).sum() / preds.shape[0]\n",
    "                cnt += 1\n",
    "            val_acc = val_acc / cnt\n",
    "            val_loss = val_loss / cnt\n",
    "\n",
    "            print('Validation loss and acc: ', val_loss.item(), val_acc.item())\n",
    "            test_acc.append(val_acc)\n",
    "\n",
    "        # model = prune_mag(model, d)\n",
    "        # print('pruned model at density: ', d)\n",
    "        model_before_prune = copy.deepcopy(model)\n",
    "        \n",
    "        if pruner == 'random':\n",
    "            model = prune_random(model, d)\n",
    "            print('pruned randomly model at density: ', d)\n",
    "        if pruner == 'mag':\n",
    "            model = prune_mag(model, d)\n",
    "            print('pruned model by magnitude at density: ', d)\n",
    "        # reset weights\n",
    "        if reset_weight:\n",
    "            original_dict = torch.load(\"model_tabular.pt\")\n",
    "            original_weights = dict(filter(lambda v: (v[0].endswith(('.weight', '.bias'))), original_dict.items()))\n",
    "            model_dict = model.state_dict()\n",
    "            model_dict.update(original_weights)\n",
    "            model.load_state_dict(model_dict)\n",
    "\n",
    "            # Reset Optimizer and Scheduler\n",
    "            optimizer.load_state_dict(torch.load(\"optimizer_tabular.pt\"))\n",
    "            print('Weights of the model reset to initialization weights')\n",
    "\n",
    "\n",
    "    model_sparse = copy.deepcopy(model)\n",
    "\n",
    "    for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = inputs.to(torch.float32), labels.long()\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        model.eval()\n",
    "        val_acc = 0\n",
    "        val_loss = 0\n",
    "        cnt = 0\n",
    "        for i, (inputs, labels) in enumerate(testloader):\n",
    "            inputs, labels = inputs.to(torch.float32), labels.long()\n",
    "            preds = model(inputs)\n",
    "            # print(preds.argmax(dim=1), labels)\n",
    "            # print(preds)\n",
    "            val_loss += criterion(preds, labels)\n",
    "            val_acc += (preds.argmax(dim=1) == labels).sum() / preds.shape[0]\n",
    "            cnt += 1\n",
    "        val_acc = val_acc / cnt\n",
    "        val_loss = val_loss / cnt\n",
    "        test_acc.append(val_acc)\n",
    "\n",
    "        print('Validation loss and acc: ', val_loss.item(), val_acc.item())\n",
    "    print('Finished Training')\n",
    "    total_num = 0\n",
    "    total_den = 0\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, (nn.Linear)):\n",
    "            total_num += m.weight_mask.sum()\n",
    "            total_den += m.weight.numel()\n",
    "            \n",
    "    model_final = copy.deepcopy(model)\n",
    "    best_acc1 = np.array(test_acc).max()\n",
    "    sparsity = 1 - (total_num / total_den)\n",
    "    write_result_to_csv(\n",
    "        name = name,\n",
    "        best_acc1 = best_acc1,\n",
    "        sparsity = sparsity,\n",
    "    )\n",
    "    return model, criterion, inputs, labels, model_final, model_before_prune, model_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dc305e2-4a77-4ec9-bd32-d84d696ca7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4, 0.30000000000000004, 0.20000000000000004, 0.10000000000000003]\n",
      "imp-0.5-0.1\n",
      "[1,   100] loss: 0.024\n",
      "[1,   200] loss: 0.022\n",
      "[1,   300] loss: 0.021\n",
      "[1,   400] loss: 0.021\n",
      "[1,   500] loss: 0.021\n",
      "Validation loss and acc:  0.4315219521522522 0.792229175567627\n",
      "[2,   100] loss: 0.021\n",
      "[2,   200] loss: 0.020\n",
      "[2,   300] loss: 0.022\n",
      "[2,   400] loss: 0.021\n",
      "[2,   500] loss: 0.020\n",
      "Validation loss and acc:  0.42244836688041687 0.8007708191871643\n",
      "[3,   100] loss: 0.021\n",
      "[3,   200] loss: 0.021\n",
      "[3,   300] loss: 0.020\n",
      "[3,   400] loss: 0.021\n",
      "[3,   500] loss: 0.020\n",
      "Validation loss and acc:  0.4161424934864044 0.7973541617393494\n",
      "[4,   100] loss: 0.020\n",
      "[4,   200] loss: 0.020\n",
      "[4,   300] loss: 0.020\n",
      "[4,   400] loss: 0.021\n",
      "[4,   500] loss: 0.020\n",
      "Validation loss and acc:  0.4362751841545105 0.7943541407585144\n",
      "[5,   100] loss: 0.021\n",
      "[5,   200] loss: 0.021\n",
      "[5,   300] loss: 0.020\n",
      "[5,   400] loss: 0.020\n",
      "[5,   500] loss: 0.021\n",
      "Validation loss and acc:  0.4350973069667816 0.8146249651908875\n",
      "[6,   100] loss: 0.021\n",
      "[6,   200] loss: 0.020\n",
      "[6,   300] loss: 0.021\n",
      "[6,   400] loss: 0.020\n",
      "[6,   500] loss: 0.021\n",
      "Validation loss and acc:  0.3958210051059723 0.8114792108535767\n",
      "[7,   100] loss: 0.020\n",
      "[7,   200] loss: 0.020\n",
      "[7,   300] loss: 0.021\n",
      "[7,   400] loss: 0.020\n",
      "[7,   500] loss: 0.020\n",
      "Validation loss and acc:  0.5395218133926392 0.8175208568572998\n",
      "[8,   100] loss: 0.020\n",
      "[8,   200] loss: 0.020\n",
      "[8,   300] loss: 0.020\n",
      "[8,   400] loss: 0.020\n",
      "[8,   500] loss: 0.020\n",
      "Validation loss and acc:  0.5829132199287415 0.7917083501815796\n",
      "[9,   100] loss: 0.019\n",
      "[9,   200] loss: 0.020\n",
      "[9,   300] loss: 0.020\n",
      "[9,   400] loss: 0.020\n",
      "[9,   500] loss: 0.021\n",
      "Validation loss and acc:  0.39580145478248596 0.8210625052452087\n",
      "[10,   100] loss: 0.020\n",
      "[10,   200] loss: 0.021\n",
      "[10,   300] loss: 0.020\n",
      "[10,   400] loss: 0.020\n",
      "[10,   500] loss: 0.020\n",
      "Validation loss and acc:  3.414243459701538 0.8119791746139526\n",
      "Overall model density after magnitude pruning at current iteration =  tensor(0.4000)\n",
      "pruned model by magnitude at density:  0.4\n",
      "[1,   100] loss: 0.020\n",
      "[1,   200] loss: 0.020\n",
      "[1,   300] loss: 0.020\n",
      "[1,   400] loss: 0.020\n",
      "[1,   500] loss: 0.020\n",
      "Validation loss and acc:  0.4004042446613312 0.8105624914169312\n",
      "[2,   100] loss: 0.020\n",
      "[2,   200] loss: 0.021\n",
      "[2,   300] loss: 0.020\n",
      "[2,   400] loss: 0.020\n",
      "[2,   500] loss: 0.019\n",
      "Validation loss and acc:  0.3925219178199768 0.8201250433921814\n",
      "[3,   100] loss: 0.020\n",
      "[3,   200] loss: 0.020\n",
      "[3,   300] loss: 0.021\n",
      "[3,   400] loss: 0.020\n",
      "[3,   500] loss: 0.020\n",
      "Validation loss and acc:  1.2070461511611938 0.8159791827201843\n",
      "[4,   100] loss: 0.020\n",
      "[4,   200] loss: 0.020\n",
      "[4,   300] loss: 0.020\n",
      "[4,   400] loss: 0.020\n",
      "[4,   500] loss: 0.019\n",
      "Validation loss and acc:  0.39600419998168945 0.8190833330154419\n",
      "[5,   100] loss: 0.020\n",
      "[5,   200] loss: 0.020\n",
      "[5,   300] loss: 0.020\n",
      "[5,   400] loss: 0.020\n",
      "[5,   500] loss: 0.020\n",
      "Validation loss and acc:  0.3925565779209137 0.8159791827201843\n",
      "[6,   100] loss: 0.020\n",
      "[6,   200] loss: 0.020\n",
      "[6,   300] loss: 0.020\n",
      "[6,   400] loss: 0.020\n",
      "[6,   500] loss: 0.020\n",
      "Validation loss and acc:  0.38972708582878113 0.8162708282470703\n",
      "[7,   100] loss: 0.019\n",
      "[7,   200] loss: 0.020\n",
      "[7,   300] loss: 0.020\n",
      "[7,   400] loss: 0.020\n",
      "[7,   500] loss: 0.020\n",
      "Validation loss and acc:  10.850043296813965 0.8161875009536743\n",
      "[8,   100] loss: 0.020\n",
      "[8,   200] loss: 0.020\n",
      "[8,   300] loss: 0.020\n",
      "[8,   400] loss: 0.020\n",
      "[8,   500] loss: 0.020\n",
      "Validation loss and acc:  0.39766421914100647 0.8138958215713501\n",
      "[9,   100] loss: 0.020\n",
      "[9,   200] loss: 0.020\n",
      "[9,   300] loss: 0.020\n",
      "[9,   400] loss: 0.020\n",
      "[9,   500] loss: 0.020\n",
      "Validation loss and acc:  0.40285253524780273 0.8109375238418579\n",
      "[10,   100] loss: 0.020\n",
      "[10,   200] loss: 0.020\n",
      "[10,   300] loss: 0.020\n",
      "[10,   400] loss: 0.020\n",
      "[10,   500] loss: 0.020\n",
      "Validation loss and acc:  14.179180145263672 0.8176250457763672\n",
      "Overall model density after magnitude pruning at current iteration =  tensor(0.3000)\n",
      "pruned model by magnitude at density:  0.30000000000000004\n",
      "[1,   100] loss: 0.019\n",
      "[1,   200] loss: 0.020\n",
      "[1,   300] loss: 0.019\n",
      "[1,   400] loss: 0.020\n",
      "[1,   500] loss: 0.020\n",
      "Validation loss and acc:  0.8548420071601868 0.8058958649635315\n",
      "[2,   100] loss: 0.020\n",
      "[2,   200] loss: 0.020\n",
      "[2,   300] loss: 0.020\n",
      "[2,   400] loss: 0.020\n",
      "[2,   500] loss: 0.019\n",
      "Validation loss and acc:  0.7809786796569824 0.8190000057220459\n",
      "[3,   100] loss: 0.020\n",
      "[3,   200] loss: 0.019\n",
      "[3,   300] loss: 0.020\n",
      "[3,   400] loss: 0.020\n",
      "[3,   500] loss: 0.019\n",
      "Validation loss and acc:  5.566444396972656 0.8194166421890259\n",
      "[4,   100] loss: 0.019\n",
      "[4,   200] loss: 0.020\n",
      "[4,   300] loss: 0.020\n",
      "[4,   400] loss: 0.020\n",
      "[4,   500] loss: 0.020\n",
      "Validation loss and acc:  0.40430089831352234 0.8197083473205566\n",
      "[5,   100] loss: 0.019\n",
      "[5,   200] loss: 0.020\n",
      "[5,   300] loss: 0.019\n",
      "[5,   400] loss: 0.020\n",
      "[5,   500] loss: 0.020\n",
      "Validation loss and acc:  0.3960493206977844 0.8190833330154419\n",
      "[6,   100] loss: 0.020\n",
      "[6,   200] loss: 0.020\n",
      "[6,   300] loss: 0.019\n",
      "[6,   400] loss: 0.019\n",
      "[6,   500] loss: 0.020\n",
      "Validation loss and acc:  0.6737360954284668 0.8167083263397217\n",
      "[7,   100] loss: 0.020\n",
      "[7,   200] loss: 0.020\n",
      "[7,   300] loss: 0.019\n",
      "[7,   400] loss: 0.020\n",
      "[7,   500] loss: 0.020\n",
      "Validation loss and acc:  0.6920512914657593 0.8158749938011169\n",
      "[8,   100] loss: 0.020\n",
      "[8,   200] loss: 0.020\n",
      "[8,   300] loss: 0.020\n",
      "[8,   400] loss: 0.020\n",
      "[8,   500] loss: 0.020\n",
      "Validation loss and acc:  0.5607897639274597 0.8066666722297668\n",
      "[9,   100] loss: 0.019\n",
      "[9,   200] loss: 0.019\n",
      "[9,   300] loss: 0.020\n",
      "[9,   400] loss: 0.020\n",
      "[9,   500] loss: 0.020\n",
      "Validation loss and acc:  0.4813274145126343 0.8170000314712524\n",
      "[10,   100] loss: 0.020\n",
      "[10,   200] loss: 0.020\n",
      "[10,   300] loss: 0.020\n",
      "[10,   400] loss: 0.020\n",
      "[10,   500] loss: 0.020\n",
      "Validation loss and acc:  0.395126610994339 0.8195000290870667\n",
      "Overall model density after magnitude pruning at current iteration =  tensor(0.2000)\n",
      "pruned model by magnitude at density:  0.20000000000000004\n",
      "[1,   100] loss: 0.019\n",
      "[1,   200] loss: 0.019\n",
      "[1,   300] loss: 0.020\n",
      "[1,   400] loss: 0.020\n",
      "[1,   500] loss: 0.020\n",
      "Validation loss and acc:  0.8899579644203186 0.7979375123977661\n",
      "[2,   100] loss: 0.020\n",
      "[2,   200] loss: 0.020\n",
      "[2,   300] loss: 0.019\n",
      "[2,   400] loss: 0.020\n",
      "[2,   500] loss: 0.020\n",
      "Validation loss and acc:  0.39816826581954956 0.8185625076293945\n",
      "[3,   100] loss: 0.020\n",
      "[3,   200] loss: 0.020\n",
      "[3,   300] loss: 0.019\n",
      "[3,   400] loss: 0.020\n",
      "[3,   500] loss: 0.020\n",
      "Validation loss and acc:  0.3890782594680786 0.817104160785675\n",
      "[4,   100] loss: 0.020\n",
      "[4,   200] loss: 0.020\n",
      "[4,   300] loss: 0.019\n",
      "[4,   400] loss: 0.020\n",
      "[4,   500] loss: 0.019\n",
      "Validation loss and acc:  0.38632073998451233 0.8213541507720947\n",
      "[5,   100] loss: 0.020\n",
      "[5,   200] loss: 0.019\n",
      "[5,   300] loss: 0.020\n",
      "[5,   400] loss: 0.020\n",
      "[5,   500] loss: 0.020\n",
      "Validation loss and acc:  0.39397749304771423 0.8147291541099548\n",
      "[6,   100] loss: 0.019\n",
      "[6,   200] loss: 0.020\n",
      "[6,   300] loss: 0.020\n",
      "[6,   400] loss: 0.019\n",
      "[6,   500] loss: 0.020\n",
      "Validation loss and acc:  0.397908478975296 0.8196041584014893\n",
      "[7,   100] loss: 0.019\n",
      "[7,   200] loss: 0.020\n",
      "[7,   300] loss: 0.020\n",
      "[7,   400] loss: 0.019\n",
      "[7,   500] loss: 0.020\n",
      "Validation loss and acc:  0.39226678013801575 0.8160833120346069\n",
      "[8,   100] loss: 0.019\n",
      "[8,   200] loss: 0.020\n",
      "[8,   300] loss: 0.020\n",
      "[8,   400] loss: 0.019\n",
      "[8,   500] loss: 0.019\n",
      "Validation loss and acc:  0.38262879848480225 0.8207291960716248\n",
      "[9,   100] loss: 0.019\n",
      "[9,   200] loss: 0.019\n",
      "[9,   300] loss: 0.020\n",
      "[9,   400] loss: 0.019\n",
      "[9,   500] loss: 0.020\n",
      "Validation loss and acc:  0.3857908844947815 0.823479175567627\n",
      "[10,   100] loss: 0.020\n",
      "[10,   200] loss: 0.019\n",
      "[10,   300] loss: 0.020\n",
      "[10,   400] loss: 0.019\n",
      "[10,   500] loss: 0.019\n",
      "Validation loss and acc:  0.43019378185272217 0.812041699886322\n",
      "Overall model density after magnitude pruning at current iteration =  tensor(0.1000)\n",
      "pruned model by magnitude at density:  0.10000000000000003\n",
      "[1,   100] loss: 0.034\n",
      "[1,   200] loss: 0.034\n",
      "[1,   300] loss: 0.034\n",
      "[1,   400] loss: 0.034\n",
      "[1,   500] loss: 0.034\n",
      "Validation loss and acc:  0.6776028871536255 0.5880624651908875\n",
      "[2,   100] loss: 0.034\n",
      "[2,   200] loss: 0.034\n",
      "[2,   300] loss: 0.034\n",
      "[2,   400] loss: 0.034\n",
      "[2,   500] loss: 0.034\n",
      "Validation loss and acc:  0.6777667999267578 0.5880624651908875\n",
      "[3,   100] loss: 0.034\n",
      "[3,   200] loss: 0.034\n",
      "[3,   300] loss: 0.034\n",
      "[3,   400] loss: 0.034\n",
      "[3,   500] loss: 0.034\n",
      "Validation loss and acc:  0.677564799785614 0.5880624651908875\n",
      "[4,   100] loss: 0.034\n",
      "[4,   200] loss: 0.034\n",
      "[4,   300] loss: 0.034\n",
      "[4,   400] loss: 0.034\n",
      "[4,   500] loss: 0.034\n",
      "Validation loss and acc:  0.6776888370513916 0.5880624651908875\n",
      "[5,   100] loss: 0.034\n",
      "[5,   200] loss: 0.034\n",
      "[5,   300] loss: 0.034\n",
      "[5,   400] loss: 0.034\n",
      "[5,   500] loss: 0.034\n",
      "Validation loss and acc:  0.6775808334350586 0.5880624651908875\n",
      "[6,   100] loss: 0.034\n",
      "[6,   200] loss: 0.034\n",
      "[6,   300] loss: 0.034\n",
      "[6,   400] loss: 0.034\n",
      "[6,   500] loss: 0.034\n",
      "Validation loss and acc:  0.6776790618896484 0.5880624651908875\n",
      "[7,   100] loss: 0.034\n",
      "[7,   200] loss: 0.034\n",
      "[7,   300] loss: 0.034\n",
      "[7,   400] loss: 0.034\n",
      "[7,   500] loss: 0.034\n",
      "Validation loss and acc:  0.6781101822853088 0.5880624651908875\n",
      "[8,   100] loss: 0.034\n",
      "[8,   200] loss: 0.034\n",
      "[8,   300] loss: 0.034\n",
      "[8,   400] loss: 0.034\n",
      "[8,   500] loss: 0.034\n",
      "Validation loss and acc:  0.6782165169715881 0.5880624651908875\n",
      "[9,   100] loss: 0.034\n",
      "[9,   200] loss: 0.034\n",
      "[9,   300] loss: 0.034\n",
      "[9,   400] loss: 0.034\n",
      "[9,   500] loss: 0.034\n",
      "Validation loss and acc:  0.6779357194900513 0.5880624651908875\n",
      "[10,   100] loss: 0.034\n",
      "[10,   200] loss: 0.034\n",
      "[10,   300] loss: 0.034\n",
      "[10,   400] loss: 0.034\n",
      "[10,   500] loss: 0.034\n",
      "Validation loss and acc:  0.6775611042976379 0.5880624651908875\n",
      "[11,   100] loss: 0.034\n",
      "[11,   200] loss: 0.034\n",
      "[11,   300] loss: 0.034\n",
      "[11,   400] loss: 0.034\n",
      "[11,   500] loss: 0.034\n",
      "Validation loss and acc:  0.6782479286193848 0.5880624651908875\n",
      "[12,   100] loss: 0.034\n",
      "[12,   200] loss: 0.034\n",
      "[12,   300] loss: 0.034\n",
      "[12,   400] loss: 0.034\n",
      "[12,   500] loss: 0.034\n",
      "Validation loss and acc:  0.6778867840766907 0.5880624651908875\n",
      "[13,   100] loss: 0.034\n",
      "[13,   200] loss: 0.034\n",
      "[13,   300] loss: 0.034\n",
      "[13,   400] loss: 0.034\n",
      "[13,   500] loss: 0.034\n",
      "Validation loss and acc:  0.6776378154754639 0.5880624651908875\n",
      "[14,   100] loss: 0.034\n",
      "[14,   200] loss: 0.034\n",
      "[14,   300] loss: 0.034\n",
      "[14,   400] loss: 0.034\n",
      "[14,   500] loss: 0.034\n",
      "Validation loss and acc:  0.6778919696807861 0.5880624651908875\n",
      "[15,   100] loss: 0.034\n",
      "[15,   200] loss: 0.034\n",
      "[15,   300] loss: 0.034\n",
      "[15,   400] loss: 0.034\n",
      "[15,   500] loss: 0.034\n",
      "Validation loss and acc:  0.6775622367858887 0.5880624651908875\n",
      "[16,   100] loss: 0.034\n",
      "[16,   200] loss: 0.034\n",
      "[16,   300] loss: 0.034\n",
      "[16,   400] loss: 0.034\n",
      "[16,   500] loss: 0.034\n",
      "Validation loss and acc:  0.6778206825256348 0.5880624651908875\n",
      "[17,   100] loss: 0.034\n",
      "[17,   200] loss: 0.034\n",
      "[17,   300] loss: 0.034\n",
      "[17,   400] loss: 0.034\n",
      "[17,   500] loss: 0.034\n",
      "Validation loss and acc:  0.6781440377235413 0.5880624651908875\n",
      "[18,   100] loss: 0.034\n",
      "[18,   200] loss: 0.034\n",
      "[18,   300] loss: 0.034\n",
      "[18,   400] loss: 0.034\n",
      "[18,   500] loss: 0.034\n",
      "Validation loss and acc:  0.6775752305984497 0.5880624651908875\n",
      "[19,   100] loss: 0.034\n",
      "[19,   200] loss: 0.034\n",
      "[19,   300] loss: 0.034\n",
      "[19,   400] loss: 0.034\n",
      "[19,   500] loss: 0.034\n",
      "Validation loss and acc:  0.6790775060653687 0.5880624651908875\n",
      "[20,   100] loss: 0.034\n",
      "[20,   200] loss: 0.034\n",
      "[20,   300] loss: 0.034\n",
      "[20,   400] loss: 0.034\n",
      "[20,   500] loss: 0.034\n",
      "Validation loss and acc:  0.6775660514831543 0.5880624651908875\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "name = ['imp']\n",
    "start_density = [0.8, 0.4]\n",
    "target_density = [0.1]\n",
    "er_list = [1, 0.5]\n",
    "cnt = 1\n",
    "for n in name:\n",
    "    for d in target_density:\n",
    "        threshold_list = []\n",
    "        s = start_density[cnt]\n",
    "        while s >= d:\n",
    "            threshold_list.append(s)\n",
    "            s = s - 0.1\n",
    "        print(threshold_list)\n",
    "        exp_name = n + '-' + str(er_list[cnt]) + '-'+ str(d) \n",
    "        if n == 'imp' or n == 'finetune':\n",
    "            pruner = 'mag'\n",
    "        \n",
    "        print(exp_name)\n",
    "        model, criterion, inputs, labels, model_final, model_before_prune, model_sparse = run(pruner = pruner, threshold_list = threshold_list, name=exp_name, er=er_list[cnt], reset_weight=False)\n",
    "        cnt += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
